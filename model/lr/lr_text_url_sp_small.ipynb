{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee52034",
   "metadata": {},
   "source": [
    "# TFIDF-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf6a74",
   "metadata": {},
   "source": [
    "# 1、data extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "145d1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "262ce5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attributes(filename):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        filename: the filename of the data file {user_id:{\"text\":text,\"user\":user,\"place_id\":place_id}}\n",
    "        \n",
    "    return:\n",
    "        result: attributes for one row\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as obj:\n",
    "        for line in obj.readlines():\n",
    "            data = json.loads(line)\n",
    "#             user_attr = data[]\n",
    "        for user_id in data.keys():\n",
    "            user_attr = data[user_id]['user']\n",
    "            return user_attr\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5e0c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_url(text):\n",
    "    '''\n",
    "    args:\n",
    "        text: the tweet text\n",
    "    return:\n",
    "        new_text: tweet text removed the url\n",
    "    '''\n",
    "    URL_REGEX = URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:\\'\\\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    url_lists = re.findall(URL_REGEX,text)\n",
    "    for url in url_lists:\n",
    "        new_text = text.replace(url,'')\n",
    "        text = new_text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15b07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_url(text):\n",
    "    '''\n",
    "    args:\n",
    "        text: the tweet text\n",
    "    return:\n",
    "        new_text: tweet text removed the url\n",
    "    '''\n",
    "    URL_REGEX = URL_REGEX = r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:\\'\\\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\"\n",
    "    url_lists = re.findall(URL_REGEX,text)\n",
    "#     for url in url_lists:\n",
    "#         new_text = text.replace(url,'')\n",
    "#         text = new_text\n",
    "    return url_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f80108d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wangyibo06/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove english stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tt = TweetTokenizer(strip_handles=True)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(data):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        data:  text while text include source text and reply text\n",
    "        \n",
    "    return:\n",
    "        result: text while text include source text and reply text(remove the stopwords)\n",
    "    \"\"\"\n",
    "    res = ''\n",
    "    for word in tt.tokenize(data):\n",
    "        if word.lower().strip() in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            res += ' '+word.lower()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd7e6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filename):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        data: {user_id:{\"text\":text,\"user\":user,\"place_id\":place_id}}\n",
    "     \n",
    "    return:\n",
    "        result: {text: [text1,text2,...],length:[length1,length2,...],label:[rumor or not,...] } \n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as obj:\n",
    "        for line in obj.readlines():\n",
    "            data = json.loads(line)\n",
    "    result = {}\n",
    "    text = []\n",
    "    textlabel = []\n",
    "    length = []\n",
    "    for user_id in data.keys():\n",
    "        text.append(remove_url(data[user_id]['text']))\n",
    "        textlabel.append(data[user_id]['place_id']) \n",
    "        length.append(len(remove_url(data[user_id]['text'])))\n",
    "    result['text'] = text\n",
    "    result['length'] = length\n",
    "    result['label'] = textlabel\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7890d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_rmsp(filename):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        data: {user_id:{\"text\":text,\"user\":user,\"place_id\":place_id}}\n",
    "     \n",
    "    return:\n",
    "        result: {text: [text1,text2,...],length:[length1,length2,...],label:[rumor or not,...] } \n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as obj:\n",
    "        for line in obj.readlines():\n",
    "            data = json.loads(line)\n",
    "    result = {}\n",
    "    text = []\n",
    "    textlabel = []\n",
    "    length = []\n",
    "    for user_id in data.keys():\n",
    "        text.append(remove_stopwords(remove_url(data[user_id]['text'])))\n",
    "        textlabel.append(data[user_id]['place_id']) \n",
    "        length.append(len(remove_stopwords(remove_url(data[user_id]['text']))))\n",
    "    result['text'] = text\n",
    "    result['length'] = length\n",
    "    result['label'] = textlabel\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9119ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './train_dev_data/0905_1005.txt'\n",
    "train_dev = prepare_data(file)\n",
    "demo = prepare_data_rmsp(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf1a0e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 34833134, 'id_str': '34833134', 'name': 'Daniel Hettner', 'screen_name': 'brisdan13', 'location': 'West End, Brisbane', 'description': '🧸🐝🍔😎🐱🖖🏻💅🏻👬🦄🌳🍁🍑🍕🍩☕️🧘🏼\\u200d♂️🎟🎭🎬🎹✈️🛸💉❤️♊️🇦🇺🇯🇵', 'url': 'https://t.co/HdWGH8eHne', 'entities': {'url': {'urls': [{'url': 'https://t.co/HdWGH8eHne', 'expanded_url': 'http://instagram.com/brisdan13', 'display_url': 'instagram.com/brisdan13', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 448, 'friends_count': 872, 'listed_count': 0, 'created_at': 'Fri Apr 24 03:27:42 +0000 2009', 'favourites_count': 18470, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 13630, 'lang': None, 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'B2DFDA', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme13/bg.gif', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme13/bg.gif', 'profile_background_tile': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1418075758946185217/xmEn2Xe6_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1418075758946185217/xmEn2Xe6_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/34833134/1586154192', 'profile_link_color': '1B95E0', 'profile_sidebar_border_color': 'FFFFFF', 'profile_sidebar_fill_color': 'FFFFFF', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': None, 'follow_request_sent': None, 'notifications': None, 'translator_type': 'none', 'withheld_in_countries': []}\n",
      "<class 'dict'>\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "train_data_attr = get_attributes(file)\n",
    "print(train_data_attr)\n",
    "print(type(train_data_attr))\n",
    "# print(train_data_attr)\n",
    "print(len(train_data_attr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfa34f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(extract_url(train_dev['text'][i]))\n",
    "#     print(remove_url(train_dev['text'][i]))\n",
    "#     print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76517d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#     print(train_dev['label'][i])\n",
    "# #     print(train_dev['text'][i])\n",
    "#     print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f337e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f393d3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just posted a photo @ West End</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Just posted a photo @ Caulfield North, Victori...</td>\n",
       "      <td>1707</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WATERCOLOUR ART CLASSES ONLINE  Thursday 9 Sep...</td>\n",
       "      <td>177</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just posted a photo @ Centennial Parklands  po...</td>\n",
       "      <td>970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just posted a photo @ Richmond Hill Angus  202...</td>\n",
       "      <td>188</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  length  label\n",
       "0                    Just posted a photo @ West End       31      4\n",
       "1  Just posted a photo @ Caulfield North, Victori...    1707      3\n",
       "2  WATERCOLOUR ART CLASSES ONLINE  Thursday 9 Sep...     177      1\n",
       "3  Just posted a photo @ Centennial Parklands  po...     970      1\n",
       "4  Just posted a photo @ Richmond Hill Angus  202...     188     11"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dev_df = pd.DataFrame(train_dev)\n",
    "train_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de4636af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>posted photo @ west end</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>posted photo @ caulfield north , victoria #po...</td>\n",
       "      <td>1202</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>watercolour art classes online thursday 9 sep...</td>\n",
       "      <td>173</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>posted photo @ centennial parklands posted ph...</td>\n",
       "      <td>892</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>posted photo @ richmond hill angus 2020 drop ...</td>\n",
       "      <td>133</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  length  label\n",
       "0                            posted photo @ west end      24      4\n",
       "1   posted photo @ caulfield north , victoria #po...    1202      3\n",
       "2   watercolour art classes online thursday 9 sep...     173      1\n",
       "3   posted photo @ centennial parklands posted ph...     892      1\n",
       "4   posted photo @ richmond hill angus 2020 drop ...     133     11"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df = pd.DataFrame(demo)\n",
    "# demo_df.head()\n",
    "train_dev_df = demo_df\n",
    "train_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5697aed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just posted a photo @ West End \n",
      " posted photo @ west end\n",
      "---------------------\n",
      "Just posted a photo @ Caulfield North, Victoria  #postride #lovegoodcoffee @sons_of_mary @ Sons Of Mary  #family#sunday #lovecraftbeer @bricklanebrewing @carwyncellars @carwyncollaborational…  #shabbosprep #lovecraftbeer @bayrdbrewing #ifyousmellwhatthehopiscooking #doubleipa @ Caulfield North,…  #sunday #botanicalgardens #postrun #lovegoodcoffee @cafeclaremont ☕️ @roastingwarehouse @ Royal Botanic Gard…  #walk #lovegoodcoffee @combielwood ☕️ @yarracoffee #elwood @ Combi Elwood  posted a photo @ Caulfield North, Victoria  #lovegoodcoffee @forth_brother ☕️ @5sensescoffee @ Forth Brother Cafe  #lovecraftbeer @coconspiratorsbeer #thesafecracker #hazyipa @ Caulfield North, Victoria  posted a photo @ Caulfield North, Victoria  #wcc #lovegoodcoffee @forth_brother ☕️ @5sensescoffee @ Forth Brother Cafe  #motivation #threeboullies #postride #lovegoodcoffee @juliettecoffeeandbread3143 ☕️ @inglewoodcoffee @ Arma…  #merricreekwalkingtrail @ Merri Creek Walking Track  #adventure #acaibowl #family #lovegoodbreakfast @lovabowl.acai @ Lovabowl  #postride #adventure #lovegoodcoffee @twobobcafe ☕️ @st_ali @ Two:Bob Cafe  #thursday #postride #jells #lovegoodcoffee @malvernsocial ☕️ @griffithsbros @ Malvern Social  #beeroclock #lovecraftbeer @coopersbrewery #australianipa @ Caulfield North, Victoria  #lovecraftbeer @afl #aflgrandfinal @melbournefc vs @westernbulldogs 🍺 @bricklanebrewing…  #mmm #lovegoodcoffee @sons_of_mary @ Sons Of Mary  #jells #postride #lovegoodcoffee @malvernsocial ☕️ @griffithsbros #greatwaytostarttheday @ Malvern Social  #postrun #lovegoodcoffee @forth_brother ☕️ @5sensescoffee @ Forth Brother Cafe  #postride #lovegoodcoffee @malvernsocial ☕️ @griffithsbros @ Malvern Social \n",
      " posted photo @ caulfield north , victoria #postride #lovegoodcoffee @ sons mary #family #sunday #lovecraftbeer l … #shabbosprep #lovecraftbeer #ifyousmellwhatthehopiscooking #doubleipa @ caulfield north , … #sunday #botanicalgardens #postrun #lovegoodcoffee ☕ ️ @ royal botanic gard … #walk #lovegoodcoffee ☕ ️ #elwood @ combi elwood posted photo @ caulfield north , victoria #lovegoodcoffee ☕ ️ @ forth brother cafe #lovecraftbeer #thesafecracker #hazyipa @ caulfield north , victoria posted photo @ caulfield north , victoria #wcc #lovegoodcoffee ☕ ️ @ forth brother cafe #motivation #threeboullies #postride #lovegoodcoffee ad3143 ☕ ️ @ arma … #merricreekwalkingtrail @ merri creek walking track #adventure #acaibowl #family #lovegoodbreakfast . acai @ lovabowl #postride #adventure #lovegoodcoffee ☕ ️ @ two : bob cafe #thursday #postride #jells #lovegoodcoffee ☕ ️ @ malvern social #beeroclock #lovecraftbeer #australianipa @ caulfield north , victoria #lovecraftbeer #aflgrandfinal vs 🍺 … #mmm #lovegoodcoffee @ sons mary #jells #postride #lovegoodcoffee ☕ ️ #greatwaytostarttheday @ malvern social #postrun #lovegoodcoffee ☕ ️ @ forth brother cafe #postride #lovegoodcoffee ☕ ️ @ malvern social\n",
      "---------------------\n",
      "WATERCOLOUR ART CLASSES ONLINE  Thursday 9 Sept -8.30pm ( EST) RSVP by 8 Sept  @ Sydney, Aus…  ART CLASSES ONLINE  Thursday 9 Sept -8.30pm  ( EST) RSVP by 8 Sept  @ Sydney, Au… \n",
      " watercolour art classes online thursday 9 sept -8.30 pm ( est ) rsvp 8 sept @ sydney , aus … art classes online thursday 9 sept -8.30 pm ( est ) rsvp 8 sept @ sydney , au …\n",
      "---------------------\n",
      "Just posted a photo @ Centennial Parklands  posted a photo @ Coogee, New South Wales  posted a photo @ Coogee, New South Wales  posted a photo @ Little Green Bean  posted a photo @ Centennial Parklands  posted a photo @ Coogee To Bondi Coastwalk  posted a photo @ Centennial Parklands  posted a photo @ Eksentrik Cafe  posted a photo @ Coogee, New South Wales  posted a photo @ South Coogee  posted a photo @ Little Green Bean  posted a photo @ Coogee, New South Wales  posted a photo @ Coogee, New South Wales  posted a photo @ Sydney, Australia  posted a photo @ Coogee To Bondi Coastwalk  posted a photo @ Little Green Bean  posted a photo @ Little Green Bean  posted a photo @ Coogee, New South Wales  posted a photo @ Coogee, New South Wales  posted a photo @ Village on Cloey  posted a photo @ Chargrill Charlie's  posted a photo @ Little Green Bean  posted a photo @ Sydney, Australia  posted a photo @ Coogee, New South Wales  posted a photo @ Little Green Bean \n",
      " posted photo @ centennial parklands posted photo @ coogee , new south wales posted photo @ coogee , new south wales posted photo @ little green bean posted photo @ centennial parklands posted photo @ coogee bondi coastwalk posted photo @ centennial parklands posted photo @ eksentrik cafe posted photo @ coogee , new south wales posted photo @ south coogee posted photo @ little green bean posted photo @ coogee , new south wales posted photo @ coogee , new south wales posted photo @ sydney , australia posted photo @ coogee bondi coastwalk posted photo @ little green bean posted photo @ little green bean posted photo @ coogee , new south wales posted photo @ coogee , new south wales posted photo @ village cloey posted photo @ chargrill charlie's posted photo @ little green bean posted photo @ sydney , australia posted photo @ coogee , new south wales posted photo @ little green bean\n",
      "---------------------\n",
      "Just posted a photo @ Richmond Hill Angus  2020 drop of heifers. Some of these, the heifers we are not keeping will be on auctions plus this Friday.…  posted a photo @ Richmond Hill Angus \n",
      " posted photo @ richmond hill angus 2020 drop heifers . , heifers keeping auctions plus friday . … posted photo @ richmond hill angus\n",
      "---------------------\n",
      "Just posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Windsor  posted a photo @ Windsor  posted a photo @ Windsor  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Brisbane, Queenland, Australia  posted a photo @ Brisbane, Queenland, Australia  posted a photo @ Brisbane, Queenland, Australia  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Fortitude Valley  posted a photo @ Fortitude Valley  posted a photo @ Fortitude Valley  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Cairns, Queensland, Australia  posted a photo @ Eumundi, Queensland  posted a photo @ Eumundi, Queensland  posted a photo @ Eumundi, Queensland  posted a photo @ Fortitude Valley  posted a photo @ Fortitude Valley  posted a photo @ Fortitude Valley  posted a photo @ Fortitude Valley  posted a photo @ Fortitude Valley  posted a photo @ Fortitude Valley  posted a photo @ Cairns, Northern Queensland  posted a photo @ Cairns, Northern Queensland  posted a photo @ Cairns, Northern Queensland  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer  posted a photo @ Bruno Domingues Photographer \n",
      " posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ windsor posted photo @ windsor posted photo @ windsor posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ brisbane , queenland , australia posted photo @ brisbane , queenland , australia posted photo @ brisbane , queenland , australia posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ fortitude valley posted photo @ fortitude valley posted photo @ fortitude valley posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ cairns , queensland , australia posted photo @ eumundi , queensland posted photo @ eumundi , queensland posted photo @ eumundi , queensland posted photo @ fortitude valley posted photo @ fortitude valley posted photo @ fortitude valley posted photo @ fortitude valley posted photo @ fortitude valley posted photo @ fortitude valley posted photo @ cairns , northern queensland posted photo @ cairns , northern queensland posted photo @ cairns , northern queensland posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer posted photo @ bruno domingues photographer\n",
      "---------------------\n",
      "#perfection doesn’t exist only #beautifulmoments #raindropsonflowers #mymondays #wetdays @ Ovingham  posted a photo @ Kaurna Country  #upcloseandpersonal with the #ground #mymondays @ Ovingham  #beautiful #greenspider on my #workpants #spideronme #spidersofinstagram #spider #mylifeontuesdays @ Ovingham  #dontstumbledance #stumbleanddance #mywednesdayworld #ifyourlifewasamusical #whatisyourthemesong…  #close to new local #wildlife #galah #mywednesdayworld #australiannativebirds @ Ovingham  responding with #empathy and #compassion, the #healing results affect all of us.. #listernwithanopenheart…  #patterns #itsmyfridaypeeps @ Ovingham  posted a photo @ Kaurna Country  posted a photo @ Ovingham  you get the #cutest #movieposter sent to you @creativelifeofelliot you are #adorable #starwarsfan…  having #mymorningcoffee and I see this #beautiful #weed on the ground #beautifuluglythings…  at #midday #moon #mymondays @ Ovingham  #love the #cloudformation this #afternoon I love how it is #covering the #sun just #alittle #mylifeontuesdays @ O…  #mylifeontuesdays @ Ovingham  #hitchingaride on #mycar a #beautiful #monochrome #moth #mylifeontuesdays @ Ovingham  posted a photo @ Kaurna Country  posted a photo @ Kaurna Country  #australiansigns #swoopingbirds #australiaspringtime #mywednesdayworld @ Ovingham  #bee #landed on my #workpants #beeonme #beesofinstagram #spider #mywednesdayworld @ Ovingham  from #williamshakespeare #shakespearequotes #myquotablelife #musicislife #mywednesdayworld @ Kaurn…  at #threeintheafternoon #moon #mythursdaystory @ Ovingham  #floweringweed #itsmyfridaypeeps #yellowflowers @ Ovingham  #cut my #longhair #newlook  #meonsundays @ Port Adelaide, South Australia  hate to be bitten by this ant #ants #antsofinstagram #wildlife #mylifeontuesdays @ Ovingham  #flowers #bees and #grassseeds #mythursdaystory @ Ovingham  #wishorweed #mymondays @ Kaurna Country  #nut #hardware #mymondays @ Ovingham  and #bees in the #morningsunshine #australiannativeflowers #nobeesnome #mywednesdayworld @ Ovin…  colours #rosella #australiannativebirds #mylifeontuesdays @ Ovingham  #buntingflag #mylifeontuesdays @ Ovingham  posted a photo @ Kaurna Country  posted a photo @ Kaurna Country  somehow #liveintheseboots I just don’t know how #steelcapboots for a #fashionista… \n",
      " #perfection ’ exist #beautifulmoments #raindropsonflowers #mymondays #wetdays @ ovingham posted photo @ kaurna country #upcloseandpersonal #ground #mymondays @ ovingham #beautiful #greenspider #workpants #spideronme #spidersofinstagram #spider #mylifeontuesdays @ ovingham #dontstumbledance #stumbleanddance #mywednesdayworld #ifyourlifewasamusical #whatisyourthemesong … #close new local #wildlife #galah #mywednesdayworld #australiannativebirds @ ovingham responding #empathy #compassion , #healing results affect us .. #listernwithanopenheart … #patterns #itsmyfridaypeeps @ ovingham posted photo @ kaurna country posted photo @ ovingham get #cutest #movieposter sent #adorable #starwarsfan … #mymorningcoffee see #beautiful #weed ground #beautifuluglythings … #midday #moon #mymondays @ ovingham #love #cloudformation #afternoon love #covering #sun #alittle #mylifeontuesdays @ … #mylifeontuesdays @ ovingham #hitchingaride #mycar #beautiful #monochrome #moth #mylifeontuesdays @ ovingham posted photo @ kaurna country posted photo @ kaurna country #australiansigns #swoopingbirds #australiaspringtime #mywednesdayworld @ ovingham #bee #landed #workpants #beeonme #beesofinstagram #spider #mywednesdayworld @ ovingham #williamshakespeare #shakespearequotes #myquotablelife #musicislife #mywednesdayworld @ kaurn … #threeintheafternoon #moon #mythursdaystory @ ovingham #floweringweed #itsmyfridaypeeps #yellowflowers @ ovingham #cut #longhair #newlook #meonsundays @ port adelaide , south australia hate bitten ant #ants #antsofinstagram #wildlife #mylifeontuesdays @ ovingham #flowers #bees #grassseeds #mythursdaystory @ ovingham #wishorweed #mymondays @ kaurna country #nut #hardware #mymondays @ ovingham #bees #morningsunshine #australiannativeflowers #nobeesnome #mywednesdayworld @ ovin … colours #rosella #australiannativebirds #mylifeontuesdays @ ovingham #buntingflag #mylifeontuesdays @ ovingham posted photo @ kaurna country posted photo @ kaurna country somehow #liveintheseboots ’ know #steelcapboots #fashionista …\n",
      "---------------------\n",
      "The day is here! Pre-order your copy of #SallyRooney’s new novel, #BeautifulWorldWhereAreYou and have your choice b…  SALLY!!!!! Pre-order your copy to make sure you get one when it’s out tomorrow!! 📚💕 #gertrudeandalice…  YOU TO OUR COMMUNITY! We are celebrating this 10k milestone with a competition — everything will be announced…  posted a photo @ Gertrude &amp; Alice Cafe Bookstore  daze 👖 @ Paddington, New South Wales  valentino peeps @ Sydney, Australia  posted a photo @ Sydney, Australia  posted a photo @ Gertrude &amp; Alice Cafe Bookstore  #NespressoVertuo looks good in every corner of the house! Keeping @nespresso.au close today as I begin to make m…  special gift from the house of @gucci. Because fashion is not a phase, it’s a #GucciLifestyle. 🎁💕 @ Gertrude &amp; Al…  posted a photo @ Gertrude &amp; Alice Cafe Bookstore  aussie designers to this windy day wit international accessoriezzz 🌟 @ Paddington, New South Wales  posted a photo @ Australia  from excitement. Joining my family at @DIOR to celebrate the launch of their e-boutique. We can now shop o…  posted a photo @ Roslyn Oxley9 Gallery  in @mrporter and this cap until I can get a moment to cut this hair. 🤠 @ Australia  ready for our next Book Club! This is a short and sweet read — you’ll definitely have time to finish it! Link v… ! Vegan Spring Green soup is now ready to take home! Come by and grab your lunch. 🥰💕 #gertrudeandalice @ Gertrud…  posted a photo @ Gertrude &amp; Alice Cafe Bookstore  the bestselling author of History Is All You Left Me comes another unforgettable story of life, loss and makin…  when we used to pay to isolate 💫💕 @ Australia ’ 📸 @ Sydney, Australia  SS22 TONIGHT, SEPT 24! 11PM AEST. ✉️💕 #PradaSS22 @ Sydney, Australia  OUTDOORS. #jagaus #takejagwithyou @ Sydney, Australia  is here! Swipe through for my @bally highlights — slide 2 &amp; 3 (same outfit) is my fave. Pick yours!! ❤️💕…  miss out next book club coming this week — Open Season is short and so very sweet, so there is still plenty o…  posted a photo @ Gertrude &amp; Alice Cafe Bookstore  chose this one! 🧩🐸🐢🌱☁️🌫 #diorwinter21 @ Sydney, Australia  ON THE MOON. T-MINUS 1 HOUR UNTIL #DIORSS22. @ Bondi Beach . Head to my stories and let’s discover SS22 together. 🥰 @ Sydney, Australia  👋 @ Australia \n",
      " day ! pre-order copy #sallyrooney ’ new novel , #beautifulworldwhereareyou choice b … sally ! ! ! pre-order copy make sure get one ’ tomorrow ! ! 📚 💕 #gertrudeandalice … community ! celebrating 10k milestone competition — everything announced … posted photo @ gertrude & alice cafe bookstore daze 👖 @ paddington , new south wales valentino peeps @ sydney , australia posted photo @ sydney , australia posted photo @ gertrude & alice cafe bookstore #nespressovertuo looks good every corner house ! keeping . au close today begin make … special gift house . fashion phase , ’ #guccilifestyle . 🎁 💕 @ gertrude & al … posted photo @ gertrude & alice cafe bookstore aussie designers windy day wit international accessoriezzz 🌟 @ paddington , new south wales posted photo @ australia excitement . joining family celebrate launch e-boutique . shop … posted photo @ roslyn oxley 9 gallery cap get moment cut hair . 🤠 @ australia ready next book club ! short sweet read — ’ definitely time finish ! link v … ! vegan spring green soup ready take home ! come grab lunch . 🥰 💕 #gertrudeandalice @ gertrud … posted photo @ gertrude & alice cafe bookstore bestselling author history left comes another unforgettable story life , loss makin … used pay isolate 💫 💕 @ australia ’ 📸 @ sydney , australia ss22 tonight , sept 24 ! 11pm aest . ✉ ️ 💕 #pradass22 @ sydney , australia outdoors . #jagaus #takejagwithyou @ sydney , australia ! swipe highlights — slide 2 & 3 ( outfit ) fave . pick ! ! ❤ ️ 💕 … miss next book club coming week — open season short sweet , still plenty … posted photo @ gertrude & alice cafe bookstore chose one ! 🧩 🐸 🐢 🌱 ☁ ️ 🌫 #diorwinter21 @ sydney , australia moon . t-minus 1 hour #diorss22 . @ bondi beach . head stories let ’ discover ss22 together . 🥰 @ sydney , australia 👋 @ australia\n",
      "---------------------\n",
      "Central Banks in Action This Week   the RBA's QE Tapering Face Delay This Afternoon?   Signals a Drawn Out Reduction to Pandemic Stimulus   Local Employment Report Looming Large  : Steady As She Goes   Markets Sluggish to Start the Week   Broadening Across US Economy   Employment Data Centre Stage Today   Volatility on the Menu   Aussie Jobs Lost in August   Risk Sentiment as Default Concerns Plague Chinese Developer   Meeting Tonight Holds Potential for USD Strength   Enjoys Improvement in Risk Sentiment   Reserve's Hawkish Tilt Spurs USD Demand   of US Fed Chatter Ahead this Week   Drifts Higher on Improved Risk Sentiment   Day for Data, Central Bank Chiefs in Action Tonight   Run for the Hills as Risk Sentiment Sours   Reclaims Lost Ground on Risk Reversal   In Action This Afternoon   Expected to Raise Rates at Midday  \n",
      " central banks action week rba's qe tapering face delay afternoon ? signals drawn reduction pandemic stimulus local employment report looming large : steady goes markets sluggish start week broadening across us economy employment data centre stage today volatility menu aussie jobs lost august risk sentiment default concerns plague chinese developer meeting tonight holds potential usd strength enjoys improvement risk sentiment reserve's hawkish tilt spurs usd demand us fed chatter ahead week drifts higher improved risk sentiment day data , central bank chiefs action tonight run hills risk sentiment sours reclaims lost ground risk reversal action afternoon expected raise rates midday\n",
      "---------------------\n",
      "On top of the world with my ride or die @juliecambridge1970 @ Mount Ngungun  a Sunday morning why not have your twin sister drag you up to the top of a mountain 🏔 @ Mount Ngungun  got it 👍 One Tree Hill 🥰 @ One Tree Hill Maleny  2 wheel exploration riding from Mudjimba to Point Arkwright 🚴 @ Point Arkwright  posted a photo @ Sunshine Coast, Queensland  this cycle friendly boardwalk at Mount Coolum Beach 👍 @ The Boardwalk Mount Coolum Beach  Double Jabbed 💉 @ Mooloolaba Beach  top of Mount Ngungun with @juliecambridge1970 look @mummasides no hands 😛😛😛 @ Mount Ngungun  Mackay ✈️ @ Mackay, Queensland  up to Mackay for a gig tonight then a few days downtime exploring The Great Barrier Reef 🙏❤️ @ Brisbane Air…  game refresher with the Mackay crew 🙌 @ Eimeo Pacific Hotel  Mackay what’s going on here then? 🚦🚦🚦👀👀👀 @ Mackay, Queensland  start today rolling through  sugar cane plantations and mountains with a morning fog 👍 @ Mackay, Queensland  in paradise #nofilter 💙 @ Whitsunday Islands National Park  💙 @ Whitsunday Islands National Park  conditions today aboard the @WhitsundayBullet thanks for having us 🙌 @ Whitsunday Islands National Park  mode 🙌 @ Saba Bay  tropical Queensland this kid comes running up onto the beach saying look Mum I got a jellyfish 😮😮😮😮😮 @ Whitehave…  for words about this place wow big up @jof_dnb for this one wow 😮 @ Whitehaven Beach, Whitsunday Island, Qld  me back to Whitehaven 💙 @ Whitsunday Islands National Park  your every day Australian picnic where the local Goannas come and join you for lunch (not my leg!) @ Whitehave…  posted a photo @ Saba Bay  posted a photo @ Sunshine Coast, Queensland  a last minute ride in with my twinny @juliecambridge1970 ❤️ @ Alex Skate Park  posted a photo @ Sunshine Coast, Queensland  :( @ Brisbane International Airport  out Australia ❤️✈️ @ Brisbane International Airport  Terminal like a ghost town :( Strange times to travel 😞 @ Brisbane International Airport  posted a photo @ Brisbane International Airport \n",
      " top world ride die @ mount ngungun sunday morning twin sister drag top mountain 🏔 @ mount ngungun got 👍 one tree hill 🥰 @ one tree hill maleny 2 wheel exploration riding mudjimba point arkwright 🚴 @ point arkwright posted photo @ sunshine coast , queensland cycle friendly boardwalk mount coolum beach 👍 @ boardwalk mount coolum beach double jabbed 💉 @ mooloolaba beach top mount ngungun look hands 😛 😛 😛 @ mount ngungun mackay ✈ ️ @ mackay , queensland mackay gig tonight days downtime exploring great barrier reef 🙏 ❤ ️ @ brisbane air … game refresher mackay crew 🙌 @ eimeo pacific hotel mackay ’ going ? 🚦 🚦 🚦 👀 👀 👀 @ mackay , queensland start today rolling sugar cane plantations mountains morning fog 👍 @ mackay , queensland paradise #nofilter 💙 @ whitsunday islands national park 💙 @ whitsunday islands national park conditions today aboard thanks us 🙌 @ whitsunday islands national park mode 🙌 @ saba bay tropical queensland kid comes running onto beach saying look mum got jellyfish 😮 😮 😮 @ whitehave … words place wow big one wow 😮 @ whitehaven beach , whitsunday island , qld back whitehaven 💙 @ whitsunday islands national park every day australian picnic local goannas come join lunch ( leg ! ) @ whitehave … posted photo @ saba bay posted photo @ sunshine coast , queensland last minute ride twinny ❤ ️ @ alex skate park posted photo @ sunshine coast , queensland :( @ brisbane international airport australia ❤ ️ ✈ ️ @ brisbane international airport terminal like ghost town :( strange times travel 😞 @ brisbane international airport posted photo @ brisbane international airport\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_dev_df['text'][i])\n",
    "    print(demo_df['text'][i])\n",
    "    print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7abc6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the train_dev dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_processed = train_dev_df['text']\n",
    "y_processed = train_dev_df['label']\n",
    "x_train,x_dev,y_train,y_dev = train_test_split(x_processed,y_processed,test_size = 0.3,stratify = y_processed,random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0bc503d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132\n",
      "915\n",
      "121     1\n",
      "1473    0\n",
      "1741    4\n",
      "2189    5\n",
      "2666    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train))\n",
    "print(len(x_dev))\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f320347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2132\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train.iloc[:].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af0bb028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "# from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f162ca07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_n: 2 features: 10000 score:0.3762\n",
      "max_n: 2 features: 20000 score:0.3618\n",
      "max_n: 3 features: 10000 score:0.3667\n",
      "max_n: 3 features: 20000 score:0.3610\n",
      "max_n: 4 features: 10000 score:0.3893\n",
      "max_n: 4 features: 20000 score:0.3606\n",
      "max_n: 5 features: 10000 score:0.3956\n",
      "max_n: 5 features: 20000 score:0.3631\n",
      "best_n: 5 best_features: 10000 best_score:0.3956\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "#lr ngram 4,features 10000\n",
    "from sklearn import svm\n",
    "\n",
    "best_score = 0\n",
    "best_clf = None\n",
    "best_tfidf = None\n",
    "for max_n in range(2,6):\n",
    "    for features in range(10000,30000,10000):\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, max_n), max_features=features).fit(x_train.iloc[:].values)\n",
    "        train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "        dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "        val_pred = clf.predict(dev_tfidf)\n",
    "        score = f1_score(y_dev.iloc[:].values, val_pred, average='macro')\n",
    "        print('max_n:',max_n,'features:',features,'score:%.4f'%score)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_clf = clf\n",
    "            best_max_n = max_n\n",
    "            best_features = features\n",
    "print('best_n:',best_max_n,'best_features:',best_features,'best_score:%.4f'%best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "562ae9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_n: 5 features: 1000 score:0.3867\n",
      "max_n: 5 features: 2000 score:0.3838\n",
      "max_n: 5 features: 3000 score:0.3927\n",
      "max_n: 5 features: 4000 score:0.3990\n",
      "max_n: 5 features: 5000 score:0.4045\n",
      "max_n: 5 features: 6000 score:0.3979\n",
      "max_n: 5 features: 7000 score:0.4148\n",
      "max_n: 5 features: 8000 score:0.4130\n",
      "max_n: 5 features: 9000 score:0.4008\n",
      "max_n: 5 features: 10000 score:0.3956\n",
      "max_n: 5 features: 11000 score:0.3940\n",
      "max_n: 5 features: 12000 score:0.3855\n",
      "max_n: 5 features: 13000 score:0.3849\n",
      "max_n: 5 features: 14000 score:0.3721\n",
      "max_n: 5 features: 15000 score:0.3622\n",
      "max_n: 5 features: 16000 score:0.3601\n",
      "max_n: 5 features: 17000 score:0.3678\n",
      "max_n: 5 features: 18000 score:0.3633\n",
      "max_n: 5 features: 19000 score:0.3632\n",
      "best_n: 5 best_features: 7000 best_score:0.4148\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import f1_score\n",
    "#lr ngram 4,features 10000\n",
    "from sklearn import svm\n",
    "\n",
    "best_score = 0\n",
    "best_clf = None\n",
    "best_tfidf = None\n",
    "for max_n in range(5,6):\n",
    "    for features in range(1000,20000,1000):\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, max_n), max_features=features).fit(x_train.iloc[:].values)\n",
    "        train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "        dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "        clf = LogisticRegression()\n",
    "        clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "        val_pred = clf.predict(dev_tfidf)\n",
    "        score = f1_score(y_dev.iloc[:].values, val_pred, average='macro')\n",
    "        print('max_n:',max_n,'features:',features,'score:%.4f'%score)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_clf = clf\n",
    "            best_max_n = max_n\n",
    "            best_features = features\n",
    "print('best_n:',best_max_n,'best_features:',best_features,'best_score:%.4f'%best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00c86254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_n: 5 best_features: 7000 best_score:0.4148\n"
     ]
    }
   ],
   "source": [
    "print('best_n:',best_max_n,'best_features:',best_features,'best_score:%.4f'%best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c77a8b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf-lr_ngram5_features4000_score0.40955266149151254.pkl']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "# store the model\n",
    "joblib.dump(best_clf, 'tfidf-lr_ngram{}_features{}_score{}.pkl'.format(best_max_n,best_features,best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4e9e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_max_n = 5\n",
    "best_features = 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "585c2cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_score:0.3979\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.10      0.18        20\n",
      "           0       0.83      0.22      0.34        69\n",
      "           1       0.45      0.81      0.57       214\n",
      "           2       0.67      0.11      0.18        38\n",
      "           3       0.55      0.75      0.63       187\n",
      "           4       0.65      0.43      0.52        79\n",
      "           5       0.60      0.54      0.57        95\n",
      "           6       0.00      0.00      0.00        13\n",
      "           7       0.79      0.45      0.58        51\n",
      "           8       0.74      0.43      0.55        60\n",
      "           9       0.80      0.18      0.30        22\n",
      "          10       0.00      0.00      0.00         7\n",
      "          11       0.71      0.45      0.56        11\n",
      "          12       0.53      0.36      0.43        22\n",
      "          13       1.00      0.50      0.67         6\n",
      "          14       0.92      0.55      0.69        20\n",
      "          15       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.55       915\n",
      "   macro avg       0.60      0.35      0.40       915\n",
      "weighted avg       0.61      0.55      0.52       915\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangyibo06/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/wangyibo06/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/wangyibo06/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,best_max_n+1), max_features=best_features).fit(x_train.iloc[:].values)\n",
    "train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "\n",
    "val_pred = clf.predict(dev_tfidf)\n",
    "score = f1_score(y_dev.iloc[:].values, val_pred, average='macro')\n",
    "\n",
    "print('best_score:%.4f'%score)\n",
    "print(metrics.classification_report(y_dev.iloc[:].values,val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4022b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare for the test data\n",
    "def prepare_test(filename):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        data: {user_id:{\"text\":text,\"user\":user,\"place_id\":place_id}}\n",
    "     \n",
    "    return:\n",
    "        result: {text: [text1,text2,...],length:[length1,length2,...],label:[rumor or not,...] } \n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as obj:\n",
    "        for line in obj.readlines():\n",
    "            data = json.loads(line)\n",
    "    result = {}\n",
    "    text = []\n",
    "    location = []\n",
    "    length = []\n",
    "    user_ids = []\n",
    "    for user_id in data.keys():\n",
    "        user_ids.append(user_id)\n",
    "        text.append(data[user_id]['text'])\n",
    "        length.append(len(data[user_id]['text']))\n",
    "        location.append(data[user_id]['location'])\n",
    "    result['user_id'] = user_ids\n",
    "    result['text'] = text\n",
    "    result['length'] = length\n",
    "    result['location'] = location\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aae4041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './test_data/0905.txt'\n",
    "test = prepare_test(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3236ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276090111</td>\n",
       "      <td>@aVoice2bHrd You know how I am 😭I cut family o...</td>\n",
       "      <td>138</td>\n",
       "      <td>West Coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1499908182</td>\n",
       "      <td>@AusAndy Most people probably have one now fro...</td>\n",
       "      <td>187</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1952211163</td>\n",
       "      <td>@anassilvvaa Bora@anassilvvaa Se envolver dinh...</td>\n",
       "      <td>1948</td>\n",
       "      <td>Invicta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>788532504626012160</td>\n",
       "      <td>@60Mins Will this program be replayed?Is last ...</td>\n",
       "      <td>1284</td>\n",
       "      <td>Newcastle, New South Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1459159765</td>\n",
       "      <td>Breaking news: emergency COVID laws applying t...</td>\n",
       "      <td>133</td>\n",
       "      <td>Fortitude Valley</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id                                               text  \\\n",
       "0           276090111  @aVoice2bHrd You know how I am 😭I cut family o...   \n",
       "1          1499908182  @AusAndy Most people probably have one now fro...   \n",
       "2          1952211163  @anassilvvaa Bora@anassilvvaa Se envolver dinh...   \n",
       "3  788532504626012160  @60Mins Will this program be replayed?Is last ...   \n",
       "4          1459159765  Breaking news: emergency COVID laws applying t...   \n",
       "\n",
       "   length                    location  \n",
       "0     138                  West Coast  \n",
       "1     187     Sydney, New South Wales  \n",
       "2    1948                     Invicta  \n",
       "3    1284  Newcastle, New South Wales  \n",
       "4     133            Fortitude Valley  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6704d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 1 ... 3 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,best_max_n+1), max_features=best_features).fit(x_train.iloc[:].values)\n",
    "# train_tfidf = tfidf.transform(x_train.iloc[:].values)\n",
    "# dev_tfidf = tfidf.transform(x_dev.iloc[:].values)\n",
    "test_tfidf = tfidf.transform(test_df['text'].iloc[:].values)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_tfidf, y_train.iloc[:].values)\n",
    "\n",
    "val_pred = clf.predict(test_tfidf)\n",
    "print(val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d905d1f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "      <th>location</th>\n",
       "      <th>predict_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>276090111</td>\n",
       "      <td>@aVoice2bHrd You know how I am 😭I cut family o...</td>\n",
       "      <td>138</td>\n",
       "      <td>West Coast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1499908182</td>\n",
       "      <td>@AusAndy Most people probably have one now fro...</td>\n",
       "      <td>187</td>\n",
       "      <td>Sydney, New South Wales</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1952211163</td>\n",
       "      <td>@anassilvvaa Bora@anassilvvaa Se envolver dinh...</td>\n",
       "      <td>1948</td>\n",
       "      <td>Invicta</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>788532504626012160</td>\n",
       "      <td>@60Mins Will this program be replayed?Is last ...</td>\n",
       "      <td>1284</td>\n",
       "      <td>Newcastle, New South Wales</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1459159765</td>\n",
       "      <td>Breaking news: emergency COVID laws applying t...</td>\n",
       "      <td>133</td>\n",
       "      <td>Fortitude Valley</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user_id                                               text  \\\n",
       "0           276090111  @aVoice2bHrd You know how I am 😭I cut family o...   \n",
       "1          1499908182  @AusAndy Most people probably have one now fro...   \n",
       "2          1952211163  @anassilvvaa Bora@anassilvvaa Se envolver dinh...   \n",
       "3  788532504626012160  @60Mins Will this program be replayed?Is last ...   \n",
       "4          1459159765  Breaking news: emergency COVID laws applying t...   \n",
       "\n",
       "   length                    location  predict_label  \n",
       "0     138                  West Coast              1  \n",
       "1     187     Sydney, New South Wales              3  \n",
       "2    1948                     Invicta              1  \n",
       "3    1284  Newcastle, New South Wales              3  \n",
       "4     133            Fortitude Valley              1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#construct the test output file\n",
    "test_df['predict_label'] = val_pred\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe3e2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write predictions to json\n",
    "# from collection import OrderedDict\n",
    "def write2json(filename,dataframe):\n",
    "    \"\"\"\n",
    "    args: \n",
    "        filename: the filename of the predicted data label file\n",
    "        dataframe: the dataframe of the predicted data\n",
    "    return:\n",
    "        None\n",
    "    \"\"\"\n",
    "    file_cnt = 0\n",
    "    new_dict = {}\n",
    "    with open(filename, 'w') as file:\n",
    "        \n",
    "        for index,row in dataframe.iterrows():\n",
    "            new_dict['id'] = row['user_id']\n",
    "            new_dict['place_id'] = row['predict_label']\n",
    "        \n",
    "            json_line = json.dumps(new_dict)\n",
    "            file.write(json_line+'\\n')\n",
    "            file_cnt += 1\n",
    "            if file_cnt % 100 == 0:\n",
    "                print('file:'+str(file_cnt))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d3db04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:100\n",
      "file:200\n",
      "file:300\n",
      "file:400\n",
      "file:500\n",
      "file:600\n",
      "file:700\n",
      "file:800\n",
      "file:900\n",
      "file:1000\n",
      "file:1100\n",
      "file:1200\n",
      "file:1300\n",
      "file:1400\n",
      "file:1500\n",
      "file:1600\n",
      "file:1700\n",
      "file:1800\n",
      "file:1900\n",
      "file:2000\n",
      "file:2100\n",
      "file:2200\n",
      "file:2300\n",
      "file:2400\n",
      "file:2500\n",
      "file:2600\n",
      "file:2700\n",
      "file:2800\n",
      "file:2900\n",
      "file:3000\n",
      "file:3100\n",
      "file:3200\n",
      "file:3300\n",
      "file:3400\n",
      "file:3500\n",
      "file:3600\n",
      "file:3700\n",
      "file:3800\n",
      "file:3900\n",
      "file:4000\n",
      "file:4100\n",
      "file:4200\n",
      "file:4300\n",
      "file:4400\n",
      "file:4500\n",
      "file:4600\n",
      "file:4700\n",
      "file:4800\n",
      "file:4900\n",
      "file:5000\n",
      "file:5100\n",
      "file:5200\n",
      "file:5300\n",
      "file:5400\n",
      "file:5500\n",
      "file:5600\n",
      "file:5700\n",
      "file:5800\n",
      "file:5900\n",
      "file:6000\n",
      "file:6100\n",
      "file:6200\n",
      "file:6300\n",
      "file:6400\n",
      "file:6500\n",
      "file:6600\n",
      "file:6700\n",
      "file:6800\n",
      "file:6900\n",
      "file:7000\n",
      "file:7100\n",
      "file:7200\n",
      "file:7300\n",
      "file:7400\n",
      "file:7500\n",
      "file:7600\n",
      "file:7700\n",
      "file:7800\n",
      "file:7900\n",
      "file:8000\n",
      "file:8100\n",
      "file:8200\n",
      "file:8300\n",
      "file:8400\n",
      "file:8500\n",
      "file:8600\n",
      "file:8700\n",
      "file:8800\n",
      "file:8900\n",
      "file:9000\n",
      "file:9100\n",
      "file:9200\n",
      "file:9300\n",
      "file:9400\n",
      "file:9500\n",
      "file:9600\n",
      "file:9700\n",
      "file:9800\n",
      "file:9900\n",
      "file:10000\n",
      "file:10100\n",
      "file:10200\n",
      "file:10300\n",
      "file:10400\n",
      "file:10500\n",
      "file:10600\n",
      "file:10700\n",
      "file:10800\n",
      "file:10900\n",
      "file:11000\n",
      "file:11100\n",
      "file:11200\n",
      "file:11300\n",
      "file:11400\n",
      "file:11500\n",
      "file:11600\n",
      "file:11700\n",
      "file:11800\n",
      "file:11900\n",
      "file:12000\n",
      "file:12100\n",
      "file:12200\n",
      "file:12300\n",
      "file:12400\n",
      "file:12500\n",
      "file:12600\n",
      "file:12700\n",
      "file:12800\n",
      "file:12900\n",
      "file:13000\n",
      "file:13100\n",
      "file:13200\n",
      "file:13300\n",
      "file:13400\n",
      "file:13500\n",
      "file:13600\n",
      "file:13700\n",
      "file:13800\n",
      "file:13900\n",
      "file:14000\n",
      "file:14100\n",
      "file:14200\n",
      "file:14300\n",
      "file:14400\n",
      "file:14500\n",
      "file:14600\n",
      "file:14700\n",
      "file:14800\n",
      "file:14900\n",
      "file:15000\n",
      "file:15100\n",
      "file:15200\n",
      "file:15300\n",
      "file:15400\n",
      "file:15500\n",
      "file:15600\n",
      "file:15700\n",
      "file:15800\n",
      "file:15900\n",
      "file:16000\n",
      "file:16100\n",
      "file:16200\n",
      "file:16300\n",
      "file:16400\n",
      "file:16500\n",
      "file:16600\n",
      "file:16700\n",
      "file:16800\n",
      "file:16900\n",
      "file:17000\n",
      "file:17100\n",
      "file:17200\n",
      "file:17300\n",
      "file:17400\n",
      "file:17500\n",
      "file:17600\n",
      "file:17700\n",
      "file:17800\n",
      "file:17900\n",
      "file:18000\n",
      "file:18100\n",
      "file:18200\n",
      "file:18300\n",
      "file:18400\n",
      "file:18500\n",
      "file:18600\n",
      "file:18700\n",
      "file:18800\n",
      "file:18900\n",
      "file:19000\n",
      "file:19100\n",
      "file:19200\n",
      "file:19300\n",
      "file:19400\n",
      "file:19500\n",
      "file:19600\n",
      "file:19700\n",
      "file:19800\n",
      "file:19900\n",
      "file:20000\n",
      "file:20100\n",
      "file:20200\n",
      "file:20300\n",
      "file:20400\n",
      "file:20500\n",
      "file:20600\n",
      "file:20700\n",
      "file:20800\n",
      "file:20900\n",
      "file:21000\n",
      "file:21100\n",
      "file:21200\n",
      "file:21300\n",
      "file:21400\n",
      "file:21500\n",
      "file:21600\n",
      "file:21700\n",
      "file:21800\n",
      "file:21900\n",
      "file:22000\n",
      "file:22100\n",
      "file:22200\n",
      "file:22300\n",
      "file:22400\n",
      "file:22500\n",
      "file:22600\n",
      "file:22700\n",
      "file:22800\n",
      "file:22900\n",
      "file:23000\n",
      "file:23100\n",
      "file:23200\n",
      "file:23300\n",
      "file:23400\n",
      "file:23500\n",
      "file:23600\n",
      "file:23700\n",
      "file:23800\n",
      "file:23900\n",
      "file:24000\n",
      "file:24100\n",
      "file:24200\n",
      "file:24300\n",
      "file:24400\n",
      "file:24500\n",
      "file:24600\n",
      "file:24700\n",
      "file:24800\n",
      "file:24900\n",
      "file:25000\n",
      "file:25100\n",
      "file:25200\n",
      "file:25300\n",
      "file:25400\n",
      "file:25500\n",
      "file:25600\n",
      "file:25700\n",
      "file:25800\n",
      "file:25900\n",
      "file:26000\n",
      "file:26100\n",
      "file:26200\n",
      "file:26300\n",
      "file:26400\n",
      "file:26500\n",
      "file:26600\n",
      "file:26700\n",
      "file:26800\n",
      "file:26900\n",
      "file:27000\n",
      "file:27100\n",
      "file:27200\n",
      "file:27300\n",
      "file:27400\n",
      "file:27500\n",
      "file:27600\n",
      "file:27700\n",
      "file:27800\n",
      "file:27900\n",
      "file:28000\n",
      "file:28100\n",
      "file:28200\n",
      "file:28300\n",
      "file:28400\n",
      "file:28500\n",
      "file:28600\n",
      "file:28700\n",
      "file:28800\n",
      "file:28900\n",
      "file:29000\n",
      "file:29100\n",
      "file:29200\n",
      "file:29300\n",
      "file:29400\n",
      "file:29500\n",
      "file:29600\n",
      "file:29700\n",
      "file:29800\n",
      "file:29900\n",
      "file:30000\n",
      "file:30100\n",
      "file:30200\n",
      "file:30300\n",
      "file:30400\n",
      "file:30500\n",
      "file:30600\n",
      "file:30700\n",
      "file:30800\n",
      "file:30900\n",
      "file:31000\n",
      "file:31100\n",
      "file:31200\n",
      "file:31300\n",
      "file:31400\n",
      "file:31500\n",
      "file:31600\n",
      "file:31700\n",
      "file:31800\n",
      "file:31900\n",
      "file:32000\n",
      "file:32100\n",
      "file:32200\n",
      "file:32300\n",
      "file:32400\n",
      "file:32500\n",
      "file:32600\n",
      "file:32700\n",
      "file:32800\n",
      "file:32900\n",
      "file:33000\n",
      "file:33100\n",
      "file:33200\n",
      "file:33300\n",
      "file:33400\n",
      "file:33500\n",
      "file:33600\n",
      "file:33700\n",
      "file:33800\n",
      "file:33900\n",
      "file:34000\n",
      "file:34100\n",
      "file:34200\n",
      "file:34300\n",
      "file:34400\n",
      "file:34500\n",
      "file:34600\n",
      "file:34700\n",
      "file:34800\n",
      "file:34900\n",
      "file:35000\n",
      "file:35100\n",
      "file:35200\n",
      "file:35300\n",
      "file:35400\n",
      "file:35500\n",
      "file:35600\n",
      "file:35700\n",
      "file:35800\n",
      "file:35900\n",
      "file:36000\n",
      "file:36100\n",
      "file:36200\n",
      "file:36300\n",
      "file:36400\n",
      "file:36500\n",
      "file:36600\n",
      "file:36700\n",
      "file:36800\n",
      "file:36900\n",
      "file:37000\n",
      "file:37100\n",
      "file:37200\n",
      "file:37300\n",
      "file:37400\n",
      "file:37500\n",
      "file:37600\n",
      "file:37700\n",
      "file:37800\n",
      "file:37900\n",
      "file:38000\n",
      "file:38100\n",
      "file:38200\n",
      "file:38300\n",
      "file:38400\n",
      "file:38500\n",
      "file:38600\n",
      "file:38700\n",
      "file:38800\n",
      "file:38900\n",
      "file:39000\n",
      "file:39100\n",
      "file:39200\n",
      "file:39300\n",
      "file:39400\n",
      "file:39500\n",
      "file:39600\n",
      "file:39700\n",
      "file:39800\n",
      "file:39900\n",
      "file:40000\n",
      "file:40100\n",
      "file:40200\n",
      "file:40300\n",
      "file:40400\n",
      "file:40500\n",
      "file:40600\n",
      "file:40700\n",
      "file:40800\n",
      "file:40900\n",
      "file:41000\n",
      "file:41100\n",
      "file:41200\n",
      "file:41300\n",
      "file:41400\n",
      "file:41500\n",
      "file:41600\n",
      "file:41700\n",
      "file:41800\n",
      "file:41900\n",
      "file:42000\n",
      "file:42100\n",
      "file:42200\n",
      "file:42300\n",
      "file:42400\n",
      "file:42500\n",
      "file:42600\n",
      "file:42700\n",
      "file:42800\n",
      "file:42900\n",
      "file:43000\n",
      "file:43100\n",
      "file:43200\n",
      "file:43300\n",
      "file:43400\n",
      "file:43500\n",
      "file:43600\n",
      "file:43700\n",
      "file:43800\n",
      "file:43900\n",
      "file:44000\n",
      "file:44100\n",
      "file:44200\n",
      "file:44300\n",
      "file:44400\n",
      "file:44500\n",
      "file:44600\n",
      "file:44700\n",
      "file:44800\n",
      "file:44900\n",
      "file:45000\n",
      "file:45100\n",
      "file:45200\n",
      "file:45300\n",
      "file:45400\n",
      "file:45500\n",
      "file:45600\n",
      "file:45700\n",
      "file:45800\n",
      "file:45900\n",
      "file:46000\n",
      "file:46100\n",
      "file:46200\n",
      "file:46300\n",
      "file:46400\n",
      "file:46500\n",
      "file:46600\n",
      "file:46700\n",
      "file:46800\n",
      "file:46900\n",
      "file:47000\n",
      "file:47100\n",
      "file:47200\n",
      "file:47300\n",
      "file:47400\n",
      "file:47500\n",
      "file:47600\n",
      "file:47700\n",
      "file:47800\n",
      "file:47900\n",
      "file:48000\n",
      "file:48100\n",
      "file:48200\n",
      "file:48300\n",
      "file:48400\n",
      "file:48500\n",
      "file:48600\n",
      "file:48700\n",
      "file:48800\n",
      "file:48900\n",
      "file:49000\n",
      "file:49100\n",
      "file:49200\n",
      "file:49300\n",
      "file:49400\n",
      "file:49500\n",
      "file:49600\n",
      "file:49700\n",
      "file:49800\n",
      "file:49900\n",
      "file:50000\n",
      "file:50100\n",
      "file:50200\n",
      "file:50300\n",
      "file:50400\n",
      "file:50500\n",
      "file:50600\n",
      "file:50700\n",
      "file:50800\n",
      "file:50900\n",
      "file:51000\n",
      "file:51100\n",
      "file:51200\n",
      "file:51300\n",
      "file:51400\n",
      "file:51500\n",
      "file:51600\n",
      "file:51700\n",
      "file:51800\n",
      "file:51900\n",
      "file:52000\n",
      "file:52100\n",
      "file:52200\n",
      "file:52300\n",
      "file:52400\n",
      "file:52500\n",
      "file:52600\n",
      "file:52700\n",
      "file:52800\n",
      "file:52900\n",
      "file:53000\n",
      "file:53100\n",
      "file:53200\n",
      "file:53300\n",
      "file:53400\n",
      "file:53500\n",
      "file:53600\n",
      "file:53700\n",
      "file:53800\n",
      "file:53900\n",
      "file:54000\n",
      "file:54100\n",
      "file:54200\n",
      "file:54300\n",
      "file:54400\n",
      "file:54500\n",
      "file:54600\n",
      "file:54700\n",
      "file:54800\n",
      "file:54900\n",
      "file:55000\n",
      "file:55100\n",
      "file:55200\n",
      "file:55300\n",
      "file:55400\n",
      "file:55500\n",
      "file:55600\n",
      "file:55700\n",
      "file:55800\n",
      "file:55900\n",
      "file:56000\n",
      "file:56100\n",
      "file:56200\n",
      "file:56300\n",
      "file:56400\n",
      "file:56500\n",
      "file:56600\n",
      "file:56700\n",
      "file:56800\n",
      "file:56900\n",
      "file:57000\n",
      "file:57100\n",
      "file:57200\n",
      "file:57300\n",
      "file:57400\n",
      "file:57500\n",
      "file:57600\n",
      "file:57700\n",
      "file:57800\n",
      "file:57900\n",
      "file:58000\n",
      "file:58100\n",
      "file:58200\n",
      "file:58300\n",
      "file:58400\n",
      "file:58500\n",
      "file:58600\n",
      "file:58700\n",
      "file:58800\n",
      "file:58900\n",
      "file:59000\n",
      "file:59100\n",
      "file:59200\n",
      "file:59300\n",
      "file:59400\n",
      "file:59500\n",
      "file:59600\n",
      "file:59700\n",
      "file:59800\n",
      "file:59900\n",
      "file:60000\n",
      "file:60100\n",
      "file:60200\n",
      "file:60300\n",
      "file:60400\n",
      "file:60500\n",
      "file:60600\n",
      "file:60700\n",
      "file:60800\n",
      "file:60900\n",
      "file:61000\n",
      "file:61100\n",
      "file:61200\n",
      "file:61300\n",
      "file:61400\n",
      "file:61500\n",
      "file:61600\n",
      "file:61700\n",
      "file:61800\n",
      "file:61900\n",
      "file:62000\n"
     ]
    }
   ],
   "source": [
    "write2json('./ml_predict/lr/lr_0905_v9.json',test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473a50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
